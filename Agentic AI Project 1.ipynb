{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe019522",
   "metadata": {},
   "source": [
    "# Agentic AI Insight System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b065b81",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "Organizations accumulate vast amounts of structured and unstructured data, but extracting actionable business insights remains slow, manual, and error-prone. There is a need for an automated, scalable, and intelligent system that can process diverse data sources, perform advanced analytics (KPI extraction, trend and anomaly detection), and deliver clear, actionable recommendations to business users in natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d1ec2",
   "metadata": {},
   "source": [
    "### Project Objective\n",
    "- Automate the end-to-end analytics workflow from data ingestion to insight delivery using a multi-agent architecture.\n",
    "\n",
    "- Enable business users to upload data and receive actionable insights (KPI, trends, anomalies, recommendations) via a simple UI or chatbot.\n",
    "\n",
    "- Leverage advanced AI (LLMs/NLP) for natural language summaries, explanations, and user Q&A.\n",
    "\n",
    "- Support multiple data types (CSV, SQL, images) and scale to various business domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c805b0",
   "metadata": {},
   "source": [
    "### Project Index\n",
    "#### 1. Intro\n",
    "\n",
    "#### 2. Problem Statement and Objectives\n",
    "\n",
    "#### 3. SIPOC Analysis\n",
    "\n",
    "#### 4. System Architecture Overview\n",
    "\n",
    "#### 5. Core Agent Implementations\n",
    "\n",
    "#### 6. Data Source Agent\n",
    "\n",
    "#### 7. Data Preprocessor Agent\n",
    "\n",
    "#### 8. Multi-Agent Processing Hub (KPI, Trend, Anomaly, Insight Writer)\n",
    "\n",
    "#### 9. Insight Delivery Agent\n",
    "\n",
    "#### 10. End-to-End Workflow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4183a42",
   "metadata": {},
   "source": [
    "### SIPOC Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd74fa3",
   "metadata": {},
   "source": [
    "### System Architecture\n",
    "\n",
    "![Architechture.png](attachment:fa6ccf63-3f92-4686-ba3a-578cbe2175d5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb853992",
   "metadata": {},
   "source": [
    "### What I Have Done\n",
    "- Designed and implemented a modular, agent-based analytics pipeline.\n",
    "\n",
    "- Enabled ingestion and preprocessing of multiple data types.\n",
    "\n",
    "- Developed specialized agents for KPI extraction, trend detection, and anomaly summarization.\n",
    "\n",
    "- Integrated LLMs (Perplexity API) for natural language explanations and recommendations.\n",
    "\n",
    "- Built an interactive UI for file upload, insight streaming, and user Q&A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad217675",
   "metadata": {},
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208c24b",
   "metadata": {},
   "source": [
    "This section introduces the notebook and outlines the technologies and libraries being used. It serves as a preamble for readers and sets the context for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (1.38.0)\n",
      "Requirement already satisfied: gradio in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (5.25.2)\n",
      "Requirement already satisfied: python-magic in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (0.4.27)\n",
      "Requirement already satisfied: pandas in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (4.13.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (2.8.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.11.6)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio-client==1.8.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-generativeai) (2.167.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-generativeai) (2.39.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\durgaprasannakompell\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "#Installing the Required Libraires\n",
    "!pip install streamlit gradio python-magic pandas matplotlib python-dotenv google-generativeai PyPDF2 Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb81011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import magic\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set your Perplexity API key\n",
    "os.environ[\"PERPLEXITY_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03f797",
   "metadata": {},
   "source": [
    "### Configuration Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04019e62",
   "metadata": {},
   "source": [
    "This code creates a configuration setup that:\n",
    "- Loads secret keys (like an API key) from a .env file\n",
    "- Allows only specific file types (like .csv, .jpg, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class to manage settings\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.ALLOWED_TYPES = ['.csv', '.xlsx', '.jpg', '.jpeg', '.png', '.sql']\n",
    "        self.MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n",
    "        self.PERPLEXITY_API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "        \n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ad6d7",
   "metadata": {},
   "source": [
    "### Data Source Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSourceAgent:\n",
    "    \"\"\"Handles loading and initial parsing of different file types\"\"\"\n",
    "    \n",
    "    def load_file(self, file_path: str) -> Tuple[Any, str]:\n",
    "        \"\"\"Load file based on its extension\"\"\"\n",
    "        extension = Path(file_path).suffix.lower()\n",
    "        \n",
    "        try:\n",
    "            if extension in ['.csv']:\n",
    "                data = pd.read_csv(file_path)\n",
    "                return data, \"csv\"\n",
    "            elif extension in ['.xlsx', '.xls']:\n",
    "                data = pd.read_excel(file_path)\n",
    "                return data, \"excel\"\n",
    "            elif extension in ['.jpg', '.jpeg', '.png']:\n",
    "                # For images, return the file path for now\n",
    "                return file_path, \"image\"\n",
    "            elif extension == '.sql':\n",
    "                # Read SQL file as text for now\n",
    "                with open(file_path, 'r') as f:\n",
    "                    sql_content = f.read()\n",
    "                return sql_content, \"sql\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {extension}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading file: {str(e)}\")\n",
    "            raise RuntimeError(f\"Failed to load file: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1661ac0",
   "metadata": {},
   "source": [
    "### Data Preprocessor Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29ae2c",
   "metadata": {},
   "source": [
    "This code is useful for the project because it automatically detects the file type and loads it accordingly. It supports multiple formats like CSV, Excel, images, and SQL files. This helps in easily handling various data sources during ETL or analysis without writing separate code each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cff4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessorAgent:\n",
    "    \"\"\"Handles data cleaning, feature engineering, and aggregation\"\"\"\n",
    "    \n",
    "    def preprocess(self, data: Any, data_type: str) -> Any:\n",
    "        \"\"\"Preprocess data based on its type\"\"\"\n",
    "        if data_type in [\"csv\", \"excel\"]:\n",
    "            return self._preprocess_dataframe(data)\n",
    "        elif data_type == \"image\":\n",
    "            return self._preprocess_image(data)\n",
    "        elif data_type == \"sql\":\n",
    "            return self._preprocess_sql(data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type for preprocessing: {data_type}\")\n",
    "    \n",
    "    def _preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess pandas DataFrame\"\"\"\n",
    "        # Basic cleaning\n",
    "        df_cleaned = df.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in df_cleaned.columns:\n",
    "            missing_pct = df_cleaned[col].isna().mean()\n",
    "            if missing_pct > 0.5:\n",
    "                # Drop columns with more than 50% missing values\n",
    "                df_cleaned = df_cleaned.drop(columns=[col])\n",
    "            elif df_cleaned[col].dtype in ['int64', 'float64']:\n",
    "                # Fill numeric columns with median\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "            else:\n",
    "                # Fill categorical/text columns with mode\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0] \n",
    "                                                        if not df_cleaned[col].mode().empty \n",
    "                                                        else \"Unknown\")\n",
    "        \n",
    "        # Feature engineering (basic example)\n",
    "        # Add date-related features if datetime columns exist\n",
    "        date_cols = [col for col in df_cleaned.columns \n",
    "                    if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        for date_col in date_cols:\n",
    "            try:\n",
    "                df_cleaned[date_col] = pd.to_datetime(df_cleaned[date_col])\n",
    "                df_cleaned[f'{date_col}_month'] = df_cleaned[date_col].dt.month\n",
    "                df_cleaned[f'{date_col}_year'] = df_cleaned[date_col].dt.year\n",
    "                df_cleaned[f'{date_col}_day'] = df_cleaned[date_col].dt.day\n",
    "            except:\n",
    "                pass  # Skip if conversion fails\n",
    "        \n",
    "        return df_cleaned\n",
    "    \n",
    "    def _preprocess_image(self, image_path: str) -> str:\n",
    "        \"\"\"Preprocess image (placeholder)\"\"\"\n",
    "        # In a real implementation, this might use computer vision libraries\n",
    "        return f\"Preprocessed image at {image_path}\"\n",
    "    \n",
    "    def _preprocess_sql(self, sql_content: str) -> str:\n",
    "        \"\"\"Preprocess SQL (placeholder)\"\"\"\n",
    "        # In a real implementation, this might validate/optimize SQL\n",
    "        return f\"Preprocessed SQL with {len(sql_content)} characters\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aeaff1",
   "metadata": {},
   "source": [
    "### KPI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f38448",
   "metadata": {},
   "source": [
    "The KPIAgent automatically extracts important metrics (KPIs) like row count, missing values, and statistics (mean, median, etc.) from data files. It currently supports structured data like CSV or Excel files. This helps in quickly understanding data quality and performance indicators without manual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09291325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPIAgent:\n",
    "    \"\"\"Extracts Key Performance Indicators from data\"\"\"\n",
    "    \n",
    "    def extract_kpis(self, data: Any, data_type: str) -> Dict:\n",
    "        \"\"\"Extract KPIs from data based on its type\"\"\"\n",
    "        if data_type in [\"csv\", \"excel\"]:\n",
    "            return self._extract_dataframe_kpis(data)\n",
    "        elif data_type == \"image\":\n",
    "            return {\"message\": \"KPI extraction from images not implemented\"}\n",
    "        elif data_type == \"sql\":\n",
    "            return {\"message\": \"KPI extraction from SQL not implemented\"}\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type for KPI extraction: {data_type}\")\n",
    "    \n",
    "    def _extract_dataframe_kpis(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extract KPIs from pandas DataFrame\"\"\"\n",
    "        kpis = {\n",
    "            \"row_count\": len(df),\n",
    "            \"column_count\": len(df.columns),\n",
    "            \"missing_values_pct\": df.isna().mean().mean() * 100,\n",
    "        }\n",
    "        \n",
    "        # Extract numeric KPIs\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            kpis[\"numeric_columns\"] = {}\n",
    "            for col in numeric_cols:\n",
    "                kpis[\"numeric_columns\"][col] = {\n",
    "                    \"mean\": df[col].mean(),\n",
    "                    \"median\": df[col].median(),\n",
    "                    \"std\": df[col].std(),\n",
    "                    \"min\": df[col].min(),\n",
    "                    \"max\": df[col].max()\n",
    "                }\n",
    "        \n",
    "        # Look for specific KPIs based on column names\n",
    "        col_lower = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Attrition rate (if relevant columns exist)\n",
    "        if any(kw in ' '.join(col_lower) for kw in ['attrition', 'churn', 'terminated']):\n",
    "            attrition_col = next((col for col in df.columns \n",
    "                                if any(kw in col.lower() \n",
    "                                      for kw in ['attrition', 'churn', 'terminated'])), None)\n",
    "            if attrition_col:\n",
    "                # Assuming binary values where 1/True/Yes indicates attrition\n",
    "                attrition_map = {'Yes': 1, 'No': 0, True: 1, False: 0, 1: 1, 0: 0}\n",
    "                if df[attrition_col].dtype == 'object':\n",
    "                    kpis[\"attrition_rate\"] = df[attrition_col].map(\n",
    "                        lambda x: attrition_map.get(x, 0) \n",
    "                        if pd.notna(x) else 0).mean() * 100\n",
    "                else:\n",
    "                    kpis[\"attrition_rate\"] = df[attrition_col].mean() * 100\n",
    "        \n",
    "        return kpis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754d31a",
   "metadata": {},
   "source": [
    "### Trend Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138e512",
   "metadata": {},
   "source": [
    "The TrendAgent helps detect important patterns over time, like month-over-month changes in key metrics. It also finds strong correlations between numeric columns in the data. This makes it easier to spot trends and relationships that may impact business performance or decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e794a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendAgent:\n",
    "    \"\"\"Detects trends in data\"\"\"\n",
    "    \n",
    "    def detect_trends(self, data: Any, data_type: str) -> Dict:\n",
    "        \"\"\"Detect trends in data based on its type\"\"\"\n",
    "        if data_type in [\"csv\", \"excel\"]:\n",
    "            return self._detect_dataframe_trends(data)\n",
    "        elif data_type == \"image\":\n",
    "            return {\"message\": \"Trend detection from images not implemented\"}\n",
    "        elif data_type == \"sql\":\n",
    "            return {\"message\": \"Trend detection from SQL not implemented\"}\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type for trend detection: {data_type}\")\n",
    "    \n",
    "    def _detect_dataframe_trends(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Detect trends in pandas DataFrame\"\"\"\n",
    "        trends = {}\n",
    "        \n",
    "        # Look for time-based columns for time series analysis\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        \n",
    "        if date_cols:\n",
    "            trends[\"time_series\"] = {}\n",
    "            for date_col in date_cols:\n",
    "                try:\n",
    "                    df[date_col] = pd.to_datetime(df[date_col])\n",
    "                    \n",
    "                    # Check for other numeric columns to analyze over time\n",
    "                    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                    \n",
    "                    for num_col in numeric_cols:\n",
    "                        # Group by month and calculate statistics\n",
    "                        monthly_data = df.groupby(df[date_col].dt.to_period(\"M\"))[num_col].agg(['mean', 'count'])\n",
    "                        \n",
    "                        # Calculate month-over-month percentage change\n",
    "                        pct_change = monthly_data['mean'].pct_change() * 100\n",
    "                        \n",
    "                        # Identify significant changes (>10% change)\n",
    "                        significant_changes = pct_change[abs(pct_change) > 10]\n",
    "                        \n",
    "                        if not significant_changes.empty:\n",
    "                            trends[\"time_series\"][f\"{num_col}_by_{date_col}\"] = {\n",
    "                                \"significant_changes\": {\n",
    "                                    str(period): change for period, change in significant_changes.items()\n",
    "                                }\n",
    "                            }\n",
    "                except:\n",
    "                    pass  # Skip if conversion fails\n",
    "        \n",
    "        # Detect correlations between numeric columns\n",
    "        numeric_df = df.select_dtypes(include=['number'])\n",
    "        if len(numeric_df.columns) > 1:\n",
    "            corr_matrix = numeric_df.corr()\n",
    "            \n",
    "            # Find strong correlations (absolute value > 0.7)\n",
    "            strong_corrs = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i):\n",
    "                    if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                        strong_corrs.append({\n",
    "                            \"col1\": corr_matrix.columns[i],\n",
    "                            \"col2\": corr_matrix.columns[j],\n",
    "                            \"correlation\": corr_matrix.iloc[i, j]\n",
    "                        })\n",
    "            \n",
    "            if strong_corrs:\n",
    "                trends[\"correlations\"] = strong_corrs\n",
    "        \n",
    "        return trends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06068bed",
   "metadata": {},
   "source": [
    "### Anomaly Agent (with Perplexity API Integration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1d246",
   "metadata": {},
   "source": [
    "The AnomalyAgent detects unusual data points (outliers) in numeric columns using statistical rules. If a Perplexity API key is available, it uses NLP to explain the anomalies and suggest business actions. This gives both technical detection and clear recommendations, helping in faster decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7003551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyAgent:\n",
    "    \"\"\"Detects anomalies and generates summaries using NLP\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initialize the Anomaly Agent with an optional API key\"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            logger.warning(\"No API key provided for Anomaly Agent. NLP summaries will be limited.\")\n",
    "    \n",
    "    def detect_anomalies(self, data: Any, data_type: str) -> Dict:\n",
    "        \"\"\"Detect anomalies in data based on its type\"\"\"\n",
    "        if data_type in [\"csv\", \"excel\"]:\n",
    "            anomalies = self._detect_dataframe_anomalies(data)\n",
    "            \n",
    "            # If API key is available, generate NLP summary\n",
    "            if self.api_key:\n",
    "                summary = self._generate_anomaly_summary(anomalies, data)\n",
    "                anomalies[\"nlp_summary\"] = summary\n",
    "            \n",
    "            return anomalies\n",
    "        elif data_type == \"image\":\n",
    "            return {\"message\": \"Anomaly detection from images not implemented\"}\n",
    "        elif data_type == \"sql\":\n",
    "            return {\"message\": \"Anomaly detection from SQL not implemented\"}\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type for anomaly detection: {data_type}\")\n",
    "    \n",
    "    def _detect_dataframe_anomalies(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Detect anomalies in pandas DataFrame\"\"\"\n",
    "        anomalies = {}\n",
    "        \n",
    "        # Detect outliers in numeric columns (using IQR method)\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        if len(numeric_cols) > 0:\n",
    "            anomalies[\"outliers\"] = {}\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                # Define outliers as values outside 1.5 * IQR from Q1 and Q3\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "                \n",
    "                if not outliers.empty:\n",
    "                    anomalies[\"outliers\"][col] = {\n",
    "                        \"count\": len(outliers),\n",
    "                        \"percentage\": len(outliers) / len(df) * 100,\n",
    "                        \"min_outlier\": outliers.min(),\n",
    "                        \"max_outlier\": outliers.max()\n",
    "                    }\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def _generate_anomaly_summary(self, anomalies: Dict, data_sample: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate NLP summary of anomalies using Perplexity API\"\"\"\n",
    "        try:\n",
    "            # Create a concise sample of the data for context\n",
    "            data_context = data_sample.head(5).to_string()\n",
    "            \n",
    "            # Prepare anomalies as a readable string\n",
    "            anomalies_text = json.dumps(anomalies, indent=2)\n",
    "            \n",
    "            # Create prompt for the model\n",
    "            prompt = f\"\"\"\n",
    "            Analyze these anomalies detected in a dataset and provide actionable recommendations:\n",
    "            \n",
    "            Data Sample (first 5 rows):\n",
    "            {data_context}\n",
    "            \n",
    "            Detected Anomalies:\n",
    "            {anomalies_text}\n",
    "            \n",
    "            Please provide:\n",
    "            1. A summary of the key anomalies\n",
    "            2. Possible business implications\n",
    "            3. 3-5 actionable recommendations\n",
    "            \"\"\"\n",
    "            \n",
    "            # Call Perplexity API\n",
    "            client = OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=\"https://api.perplexity.ai\"\n",
    "            )\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"sonar-pro-online\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a data analyst specializing in anomaly detection and business insights.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating anomaly summary: {str(e)}\")\n",
    "            return f\"Error generating summary: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456fc46",
   "metadata": {},
   "source": [
    "### Insight Writer Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc04de5",
   "metadata": {},
   "source": [
    "The InsightWriterAgent takes outputs from other agents (like KPIs, trends, anomalies) and automatically generates clear, human-readable insights. It helps turn complex data analysis into simple business narratives or reports. This is useful for non-technical stakeholders to quickly understand what's happening in the data and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsightWriterAgent:\n",
    "    \"\"\"Composes insights from KPIs, trends, and anomalies\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initialize the Insight Writer with an optional API key\"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            logger.warning(\"No API key provided for Insight Writer. NLP insights will be limited.\")\n",
    "    \n",
    "    def generate_insights(self, kpis: Dict, trends: Dict, anomalies: Dict, user_query: str) -> str:\n",
    "        \"\"\"Generate insights from KPIs, trends, and anomalies\"\"\"\n",
    "        try:\n",
    "            # If API key is available, use Perplexity for insights\n",
    "            if self.api_key:\n",
    "                return self._generate_nlp_insights(kpis, trends, anomalies, user_query)\n",
    "            else:\n",
    "                # Fallback to simple template-based insights\n",
    "                return self._generate_basic_insights(kpis, trends, anomalies)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating insights: {str(e)}\")\n",
    "            return f\"Error generating insights: {str(e)}\"\n",
    "    \n",
    "    def _generate_nlp_insights(self, kpis: Dict, trends: Dict, anomalies: Dict, user_query: str) -> str:\n",
    "        \"\"\"Generate insights using Perplexity API\"\"\"\n",
    "        try:\n",
    "            # Prepare data as readable strings\n",
    "            kpis_text = json.dumps(kpis, indent=2)\n",
    "            trends_text = json.dumps(trends, indent=2)\n",
    "            anomalies_text = json.dumps(anomalies, indent=2)\n",
    "            \n",
    "            # Create prompt for the model\n",
    "            prompt = f\"\"\"\n",
    "            Analyze this data and provide insights in response to the user's query: \"{user_query}\"\n",
    "            \n",
    "            KPIs:\n",
    "            {kpis_text}\n",
    "            \n",
    "            Trends:\n",
    "            {trends_text}\n",
    "            \n",
    "            Anomalies:\n",
    "            {anomalies_text}\n",
    "            \n",
    "            Please provide:\n",
    "            1. A summary of the key findings\n",
    "            2. Direct answers to the user's query\n",
    "            3. 3-5 actionable recommendations\n",
    "            4. Any additional insights you can derive from the data\n",
    "            \"\"\"\n",
    "            \n",
    "            # Call Perplexity API\n",
    "            client = OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=\"https://api.perplexity.ai\"\n",
    "            )\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"sonar-pro-online\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a business intelligence expert who provides clear, actionable insights.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating NLP insights: {str(e)}\")\n",
    "            return f\"Error generating NLP insights: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1491d7",
   "metadata": {},
   "source": [
    "### Insight Delivery Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b5c4c",
   "metadata": {},
   "source": [
    "The InsightDeliveryAgent is responsible for delivering insights to users and answering their follow-up questions using analyzed data (like KPIs, trends, and anomalies). It uses NLP with the Perplexity API to generate meaningful, context-aware responses. This makes the system interactive and helps users make data-driven decisions more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbaf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsightDeliveryAgent:\n",
    "    \"\"\"Delivers insights to the user\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initialize the Insight Delivery Agent with an optional API key\"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def deliver_insights(self, insights: str) -> str:\n",
    "        \"\"\"Deliver insights to the user\"\"\"\n",
    "        # In a real implementation, this might format the insights for display\n",
    "        return insights\n",
    "    \n",
    "    def answer_question(self, question: str, kpis: Dict, trends: Dict, anomalies: Dict) -> str:\n",
    "        \"\"\"Answer a follow-up question based on the analyzed data\"\"\"\n",
    "        try:\n",
    "            if not self.api_key:\n",
    "                return \"API key required for Q&A functionality.\"\n",
    "            \n",
    "            # Add question to conversation history\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "            \n",
    "            # Prepare data as readable strings\n",
    "            kpis_text = json.dumps(kpis, indent=2)\n",
    "            trends_text = json.dumps(trends, indent=2)\n",
    "            anomalies_text = json.dumps(anomalies, indent=2)\n",
    "            \n",
    "            # Create prompt for the model\n",
    "            prompt = f\"\"\"\n",
    "            Answer the user's question based on this analyzed data:\n",
    "            \n",
    "            User's question: \"{question}\"\n",
    "            \n",
    "            KPIs:\n",
    "            {kpis_text}\n",
    "            \n",
    "            Trends:\n",
    "            {trends_text}\n",
    "            \n",
    "            Anomalies:\n",
    "            {anomalies_text}\n",
    "            \n",
    "            Previous conversation:\n",
    "            {json.dumps(self.conversation_history[:-1], indent=2) if len(self.conversation_history) > 1 else \"None\"}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Call Perplexity API\n",
    "            client = OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=\"https://api.perplexity.ai\"\n",
    "            )\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"sonar-pro-online\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a data analyst assistant who answers questions based on analyzed data.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            # Add answer to conversation history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            \n",
    "            return answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error answering question: {str(e)}\")\n",
    "            return f\"Error answering question: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5ac4a",
   "metadata": {},
   "source": [
    "### Orchestration Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bafe4",
   "metadata": {},
   "source": [
    "The AgentOrchestrator is the central brain of the entire analytical pipeline. It coordinates agents like data loaders, preprocessors, KPI extractors, trend/anomaly detectors, and insight generators. This enables a seamless, automated flow from raw data ingestion to delivering actionable insights and answering user questions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentOrchestrator:\n",
    "    \"\"\"Orchestrates the agentic workflow\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initialize the orchestrator with agents\"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "        \n",
    "        # Initialize all agents\n",
    "        self.data_source_agent = DataSourceAgent()\n",
    "        self.data_preprocessor_agent = DataPreprocessorAgent()\n",
    "        self.kpi_agent = KPIAgent()\n",
    "        self.trend_agent = TrendAgent()\n",
    "        self.anomaly_agent = AnomalyAgent(api_key=self.api_key)\n",
    "        self.insight_writer_agent = InsightWriterAgent(api_key=self.api_key)\n",
    "        self.insight_delivery_agent = InsightDeliveryAgent(api_key=self.api_key)\n",
    "        \n",
    "        # Store analysis results\n",
    "        self.results = {\n",
    "            \"data\": None,\n",
    "            \"data_type\": None,\n",
    "            \"kpis\": None,\n",
    "            \"trends\": None,\n",
    "            \"anomalies\": None,\n",
    "            \"insights\": None\n",
    "        }\n",
    "    \n",
    "    def process_file(self, file_path: str, user_query: str) -> Dict:\n",
    "        \"\"\"Process a file through the entire agent pipeline\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Step 1: Load file\n",
    "            data, data_type = self.data_source_agent.load_file(file_path)\n",
    "            logger.info(f\"File loaded as {data_type}\")\n",
    "            \n",
    "            # Step 2: Preprocess data\n",
    "            preprocessed_data = self.data_preprocessor_agent.preprocess(data, data_type)\n",
    "            logger.info(\"Data preprocessing complete\")\n",
    "            \n",
    "            # Store preprocessed data\n",
    "            self.results[\"data\"] = preprocessed_data\n",
    "            self.results[\"data_type\"] = data_type\n",
    "            \n",
    "            # Step 3: Extract KPIs\n",
    "            kpis = self.kpi_agent.extract_kpis(preprocessed_data, data_type)\n",
    "            logger.info(f\"KPI extraction complete: {len(kpis)} KPIs found\")\n",
    "            self.results[\"kpis\"] = kpis\n",
    "            \n",
    "            # Step 4: Detect trends\n",
    "            trends = self.trend_agent.detect_trends(preprocessed_data, data_type)\n",
    "            logger.info(\"Trend detection complete\")\n",
    "            self.results[\"trends\"] = trends\n",
    "            \n",
    "            # Step 5: Detect anomalies\n",
    "            anomalies = self.anomaly_agent.detect_anomalies(preprocessed_data, data_type)\n",
    "            logger.info(\"Anomaly detection complete\")\n",
    "            self.results[\"anomalies\"] = anomalies\n",
    "            \n",
    "            # Step 6: Generate insights\n",
    "            insights = self.insight_writer_agent.generate_insights(\n",
    "                kpis, trends, anomalies, user_query)\n",
    "            logger.info(\"Insight generation complete\")\n",
    "            self.results[\"insights\"] = insights\n",
    "            \n",
    "            # Step 7: Deliver insights\n",
    "            formatted_insights = self.insight_delivery_agent.deliver_insights(insights)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": \"Analysis complete\",\n",
    "                \"insights\": formatted_insights\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in processing pipeline: {str(e)}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"message\": f\"Error: {str(e)}\",\n",
    "                \"insights\": None\n",
    "            }\n",
    "    \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Answer a follow-up question about the analyzed data\"\"\"\n",
    "        if not all([self.results[\"kpis\"], self.results[\"trends\"], self.results[\"anomalies\"]]):\n",
    "            return \"Please analyze data first before asking questions.\"\n",
    "        \n",
    "        return self.insight_delivery_agent.answer_question(\n",
    "            question, \n",
    "            self.results[\"kpis\"], \n",
    "            self.results[\"trends\"], \n",
    "            self.results[\"anomalies\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e41480",
   "metadata": {},
   "source": [
    "### Visualization Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ecf3f7",
   "metadata": {},
   "source": [
    "The create_visualizations function generates plots for numeric and categorical data distributions, as well as a correlation heatmap if correlation data is available. It works specifically for tabular data formats like CSV or Excel. The output includes saved PNG files that visually summarize the key patterns in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(orchestrator):\n",
    "    \"\"\"Create visualizations based on analyzed data\"\"\"\n",
    "    if orchestrator.results[\"data_type\"] not in [\"csv\", \"excel\"]:\n",
    "        return \"Visualizations only available for tabular data.\"\n",
    "    \n",
    "    df = orchestrator.results[\"data\"]\n",
    "    \n",
    "    # Set up the plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Plot numeric column distributions\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns[:4]  # Limit to first 4\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        sns.histplot(df[col].dropna(), kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('numeric_distributions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Plot categorical column distributions (if any)\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns[:4]\n",
    "    \n",
    "    if len(cat_cols) > 0:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        for i, col in enumerate(cat_cols):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            value_counts = df[col].value_counts().nlargest(10)  # Top 10 categories\n",
    "            sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "            plt.title(f'Top categories in {col}')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('categorical_distributions.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Plot correlations (if available)\n",
    "    if \"correlations\" in orchestrator.results[\"trends\"]:\n",
    "        corrs = orchestrator.results[\"trends\"][\"correlations\"]\n",
    "        if corrs:\n",
    "            corr_cols = list(set([c[\"col1\"] for c in corrs] + [c[\"col2\"] for c in corrs]))\n",
    "            if len(corr_cols) > 1:\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(df[corr_cols].corr(), annot=True, cmap='coolwarm')\n",
    "                plt.title('Correlation Matrix')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('correlations.png')\n",
    "                plt.close()\n",
    "    \n",
    "    return \"Visualizations created successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b645f",
   "metadata": {},
   "source": [
    "### Agent Orchestrator process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717cdb4",
   "metadata": {},
   "source": [
    "This script demonstrates how to use the AgentOrchestrator to process an HR dataset and analyze attrition patterns. It runs a full pipeline: data loading, analysis, insight generation, and visualization. The setup also allows follow-up questions to be answered based on the analyzed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:03:33,704 - __main__ - INFO - Processing file: C:\\Users\\DurgaPrasannaKompell\\OneDrive - SplashBI\\Desktop\\ibm data\\HR Data.csv\n",
      "2025-04-24 18:03:33,829 - __main__ - INFO - File loaded as csv\n",
      "C:\\Users\\DurgaPrasannaKompell\\AppData\\Local\\Temp\\ipykernel_20404\\2850726529.py:41: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_cleaned[date_col] = pd.to_datetime(df_cleaned[date_col])\n",
      "2025-04-24 18:03:33,968 - __main__ - INFO - Data preprocessing complete\n",
      "2025-04-24 18:03:33,998 - __main__ - INFO - KPI extraction complete: 5 KPIs found\n",
      "C:\\Users\\DurgaPrasannaKompell\\AppData\\Local\\Temp\\ipykernel_20404\\4156624309.py:26: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "2025-04-24 18:03:34,548 - __main__ - INFO - Trend detection complete\n",
      "2025-04-24 18:03:56,766 - httpx - INFO - HTTP Request: POST https://api.perplexity.ai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 18:03:56,809 - __main__ - INFO - Anomaly detection complete\n"
     ]
    }
   ],
   "source": [
    "# Example setup\n",
    "file_path = r'C:\\Users\\DurgaPrasannaKompell\\OneDrive - SplashBI\\Desktop\\ibm data\\HR Data.csv'\n",
    "user_query = \"Analyze this HR data and identify attrition patterns\"\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator = AgentOrchestrator()\n",
    "\n",
    "# Process the file\n",
    "result = orchestrator.process_file(file_path, user_query)\n",
    "\n",
    "# Check if processing was successful\n",
    "if result[\"success\"]:\n",
    "    print(\"Analysis Complete!\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(result[\"insights\"])\n",
    "\n",
    "    # Create visualizations\n",
    "    viz_status = create_visualizations(orchestrator)\n",
    "    print(f\"\\nVisualizations: {viz_status}\")\n",
    "    \n",
    "    # Ask follow-up questions\n",
    "    answer = orchestrator.answer_question(\"Which department has the highest attrition rate?\")\n",
    "    print(\"\\nFollow-up Question Answer:\")\n",
    "    print(answer)\n",
    "\n",
    "else:\n",
    "    print(f\"Analysis failed: {result['message']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7b95e",
   "metadata": {},
   "source": [
    "The analyze_with_agents function processes a dataset and performs analysis using the AgentOrchestrator class, which orchestrates the workflow of multiple agents (e.g., data source, preprocessing, KPI extraction). The function displays the generated insights directly in a Jupyter notebook using IPython's display capabilities, allowing for follow-up questions based on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of running Streamlit directly:\n",
    "def analyze_with_agents(file_path, query):\n",
    "    \"\"\"Function to use in Jupyter notebook\"\"\"\n",
    "    orchestrator = AgentOrchestrator()\n",
    "    result = orchestrator.process_file(file_path, query)\n",
    "    \n",
    "    # Display results using IPython display instead of Streamlit\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(f\"## Analysis Results\\n\\n{result['insights']}\"))\n",
    "    \n",
    "    return orchestrator  # Return for follow-up questions\n",
    "\n",
    "# Use this in a notebook cell:\n",
    "# orchestrator = analyze_with_agents(\"your_file.csv\", \"Analyze this data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991a866",
   "metadata": {},
   "source": [
    "### Integrating with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a0423",
   "metadata": {},
   "source": [
    "This Streamlit app lets users upload data files, request insights, and view AI-generated results, including visualizations and follow-up Q&A. Simply upload a file, enter a query, and the app handles everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "def create_streamlit_app():\n",
    "    st.set_page_config(page_title=\"Agentic AI Insights\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\" Agentic AI Insight System\")\n",
    "    st.write(\"Upload your data, ask questions, and get AI-powered insights.\")\n",
    "    \n",
    "    # Initialize the orchestrator\n",
    "    if \"orchestrator\" not in st.session_state:\n",
    "        api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "        st.session_state.orchestrator = AgentOrchestrator(api_key=api_key)\n",
    "    \n",
    "    # File upload\n",
    "    uploaded_file = st.file_uploader(\"Upload your data file\", \n",
    "                                    type=['csv', 'xlsx', 'jpg', 'jpeg', 'png', 'sql'])\n",
    "    user_query = st.text_area(\"What insights are you looking for?\", \n",
    "                             \"Analyze this data and provide business insights.\")\n",
    "    \n",
    "    if uploaded_file and st.button(\"Process\", type=\"primary\"):\n",
    "        with st.spinner(\"Processing your data...\"):\n",
    "            # Save uploaded file temporarily\n",
    "            temp_path = f\"temp_{uploaded_file.name}\"\n",
    "            with open(temp_path, \"wb\") as f:\n",
    "                f.write(uploaded_file.getbuffer())\n",
    "            \n",
    "            # Process the file\n",
    "            result = st.session_state.orchestrator.process_file(temp_path, user_query)\n",
    "            \n",
    "            # Display results\n",
    "            if result[\"success\"]:\n",
    "                st.success(\"Analysis complete!\")\n",
    "                \n",
    "                # Create tabs for different outputs\n",
    "                tab1, tab2, tab3 = st.tabs([\" Insights\", \" Visualizations\", \" Q&A\"])\n",
    "                \n",
    "                with tab1:\n",
    "                    st.markdown(result[\"insights\"])\n",
    "                \n",
    "                with tab2:\n",
    "                    st.write(\"### Data Visualizations\")\n",
    "                    viz_status = create_visualizations(st.session_state.orchestrator)\n",
    "                    \n",
    "                    # Display visualizations if created successfully\n",
    "                    try:\n",
    "                        col1, col2 = st.columns(2)\n",
    "                        with col1:\n",
    "                            st.image(\"numeric_distributions.png\")\n",
    "                        with col2:\n",
    "                            st.image(\"categorical_distributions.png\")\n",
    "                        \n",
    "                        st.image(\"correlations.png\")\n",
    "                    except:\n",
    "                        st.write(viz_status)\n",
    "                \n",
    "                with tab3:\n",
    "                    st.write(\"### Ask Follow-up Questions\")\n",
    "                    follow_up = st.text_input(\"What else would you like to know?\")\n",
    "                    \n",
    "                    if follow_up and st.button(\"Ask\", type=\"secondary\"):\n",
    "                        with st.spinner(\"Thinking...\"):\n",
    "                            answer = st.session_state.orchestrator.answer_question(follow_up)\n",
    "                            st.markdown(answer)\n",
    "            else:\n",
    "                st.error(f\"Analysis failed: {result['message']}\")\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            os.remove(temp_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_streamlit_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272ccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
